{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5582b4d1-a6f7-4620-9fbd-29eb61bb244f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73142169-7a56-4de7-91ff-c3f326ec651e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89], # Harry         (x^1)\n",
    "     [0.55, 0.87, 0.66], # Potter        (x^2)\n",
    "     [0.57, 0.85, 0.64], # and           (x^3)\n",
    "     [0.22, 0.58, 0.33], # the           (x^4)\n",
    "     [0.77, 0.25, 0.10], # Philosopher's (x^5)\n",
    "     [0.05, 0.80, 0.55]] # Stone         (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "351e9a79-5210-4ddb-a7ee-b0943aa59897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5500, 0.8700, 0.6600])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_query = inputs[1]\n",
    "input_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "385d910e-22cf-4913-80ac-ecbd7801c197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4300, 0.1500, 0.8900])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_1 = inputs[0]\n",
    "input_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "922fcf06-3c05-4824-9987-58aab6bd8902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9544"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.55*0.43) + (0.87*0.15) + (0.66*0.89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df703c5a-0ba6-453f-946c-851cd94ff5a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9544)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_score_21 = torch.dot(input_query, input_1)\n",
    "attention_score_21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d14c5fc-f5d1-4b10-8080-93e5c3c50824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4950)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 1\n",
    "attention = torch.dot(input_query, inputs[i])\n",
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a880010-2bec-4443-8932-3f7981033dac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores_input_2 = torch.empty(inputs.shape[0])\n",
    "for idx, item in enumerate(inputs):\n",
    "    attention_scores_input_2[idx] = torch.dot(input_query, inputs[idx])\n",
    "\n",
    "attention_scores_input_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ac7f8bf-8aad-4f9f-8309-1e94ac493928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights_temp = attention_scores_input_2 / attention_scores_input_2.sum()\n",
    "attention_weights_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2102dec1-2e49-4c6a-8f28-a17e20642271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights_temp.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08fbcb77-075c-4270-9666-d963db43a0d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "attention_weights_2 = softmax_naive(attention_scores_input_2)\n",
    "attention_weights_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6480f3b-580f-4413-9dfb-ce6e79e5f98b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights_2.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "488ac63c-160c-4c4d-b7a0-88d4e3591257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4419, 0.6515, 0.5683])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vector_2 = torch.zeros(inputs[1].shape[0])\n",
    "\n",
    "for i, item in enumerate(inputs):\n",
    "    context_vector_2 += attention_weights_2[i]*inputs[i]\n",
    "\n",
    "context_vector_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c983ebd-ff4b-419a-91a3-81c31f801e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
       "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
       "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
       "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
       "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
       "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_attention_scores = torch.empty(6,6)\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        all_attention_scores[i,j] = torch.dot(x_i, x_j)\n",
    "\n",
    "all_attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06525e39-a19a-4511-a9c2-9835677ced53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
       "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
       "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
       "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
       "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
       "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_attention_scores = inputs @ inputs.T\n",
    "all_attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d39fe6d-f914-41be-a8ff-fbfb88e3849e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
       "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
       "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
       "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
       "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
       "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_attention_weights = torch.softmax(all_attention_scores , dim=1)\n",
    "all_attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffc559e5-74f4-43d1-bea8-7c4a54a7ed39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_attention_weights.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d80da700-0599-4765-9213-e933517ac837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4421, 0.5931, 0.5790],\n",
       "        [0.4419, 0.6515, 0.5683],\n",
       "        [0.4431, 0.6496, 0.5671],\n",
       "        [0.4304, 0.6298, 0.5510],\n",
       "        [0.4671, 0.5910, 0.5266],\n",
       "        [0.4177, 0.6503, 0.5645]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_context_vectors = all_attention_weights @ inputs\n",
    "all_context_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029bc8e6-a1a4-49b5-80f3-191742fcc84e",
   "metadata": {},
   "source": [
    "## Self attention with trainable weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df8480a1-4663-46c4-8166-88b1c9d97b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b0f02f6-9992-409e-af3d-662eb47dfe64",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out))\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out))\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e56b5d00-e2d2-4138-a18e-df481a0a8207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2309, 1.0966],\n",
       "        [0.4306, 1.4551],\n",
       "        [0.4300, 1.4343],\n",
       "        [0.2355, 0.7990],\n",
       "        [0.2983, 0.6565],\n",
       "        [0.2568, 1.0533]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "querys = inputs @ W_query\n",
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "querys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e843ec6-6cf0-478b-a34d-9eedf6fccfb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4306, 1.4551], grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_2 = x_2 @ W_query\n",
    "query_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26ed5700-562b-4971-802b-f5db9865e442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.8524, grad_fn=<DotBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_2 = keys[1]\n",
    "attention_scores_22 = torch.dot(query_2, key_2)\n",
    "attention_scores_22\n",
    "# attention_scores = query_2 @ keys.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "12d2eca7-bb4f-4b0c-b7de-a10f136e2b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440],\n",
       "       grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores_2 = query_2 @ keys.T\n",
    "attention_scores_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "47021b72-151e-4b0b-8d8d-5bbe9ac200a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_k = keys.shape[1]\n",
    "attention_weights_2 = torch.softmax(attention_scores_2/ d_k**0.5, dim=-1)\n",
    "attention_weights_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "13238bea-4ce0-44e9-91ba-a582c1c6b99b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3061, 0.8210], grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vector_2 = attention_weights_2 @ values\n",
    "context_vector_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36effb7-8f9a-4694-a598-044c5050c23a",
   "metadata": {},
   "source": [
    "# Implementing a compact Self-Attention class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f5ac8212-e291-46aa-a164-73876937b62a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2996, 0.8053],\n",
       "        [0.3061, 0.8210],\n",
       "        [0.3058, 0.8203],\n",
       "        [0.2948, 0.7939],\n",
       "        [0.2927, 0.7891],\n",
       "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = torch.nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = torch.nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = torch.nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        querys = x @ self.W_query\n",
    "        keys = x @ self.W_key\n",
    "        values = x @ self.W_value\n",
    "\n",
    "        attention_scores = querys @ keys.T\n",
    "        attention_weights = torch.softmax(attention_scores /  d_k**0.5, dim=-1)\n",
    "\n",
    "        context_vectors = attention_weights @ values\n",
    "\n",
    "        return context_vectors\n",
    "        \n",
    "\n",
    "torch.manual_seed(123)\n",
    "self_attention_v1 = SelfAttention_v1(d_in, d_out)\n",
    "self_attention_v1(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9e1d64e3-426a-4a7c-bb0d-2196565c3bd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5337, -0.1051],\n",
       "        [-0.5323, -0.1080],\n",
       "        [-0.5323, -0.1079],\n",
       "        [-0.5297, -0.1076],\n",
       "        [-0.5311, -0.1066],\n",
       "        [-0.5299, -0.1081]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v2(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        querys = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attention_scores = querys @ keys.T\n",
    "        attention_weights = torch.softmax(attention_scores /  d_k**0.5, dim=-1)\n",
    "\n",
    "        context_vectors = attention_weights @ values\n",
    "\n",
    "        return context_vectors\n",
    "        \n",
    "\n",
    "torch.manual_seed(123)\n",
    "self_attention_v2 = SelfAttention_v2(d_in, d_out)\n",
    "self_attention_v2(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f575432-ffee-40d0-8dec-2011db3089d0",
   "metadata": {},
   "source": [
    "# Causal Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9a515a1d-d26c-4171-b2a9-634afe8d428e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3111, 0.3479, 0.3471, 0.1714, 0.2350, 0.1928],\n",
       "        [0.1655, 0.2602, 0.2576, 0.1445, 0.1384, 0.1790],\n",
       "        [0.1667, 0.2602, 0.2577, 0.1443, 0.1391, 0.1784],\n",
       "        [0.0510, 0.1080, 0.1064, 0.0643, 0.0476, 0.0835],\n",
       "        [0.1415, 0.1875, 0.1863, 0.0987, 0.1121, 0.1174],\n",
       "        [0.0476, 0.1192, 0.1171, 0.0731, 0.0477, 0.0966]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "querys = self_attention_v2.W_query(inputs)\n",
    "keys = self_attention_v2.W_key(inputs)\n",
    "values = self_attention_v2.W_value(inputs)\n",
    "\n",
    "attention_scores = querys @ keys.T\n",
    "attention_weights = torch.softmax(attention_scores /  d_k**0.5, dim=-1)\n",
    "attention_weights\n",
    "attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "84063c10-161f-4f61-81a0-2a975f300d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_length = attention_scores.shape[0]\n",
    "context_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a2ed858a-6962-406e-b9bc-5b39a452cea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_simple = torch.tril(torch.ones(context_length, context_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6d0e7539-9379-4f0b-8654-a07707c2934d",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_weights = attention_weights * mask_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8bb265c8-efda-4457-ad55-5ee121dcc79e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3190, 0.3408, 0.3402, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2445, 0.2545, 0.2542, 0.2468, 0.0000, 0.0000],\n",
       "        [0.1994, 0.2060, 0.2058, 0.1935, 0.1953, 0.0000],\n",
       "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows_sum = masked_weights.sum(dim=-1, keepdim=True)\n",
    "normalized_masked_weights = masked_weights / rows_sum\n",
    "normalized_masked_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4f98804d-8fa7-4937-a439-538ade43da46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3111,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
       "        [0.1655, 0.2602,   -inf,   -inf,   -inf,   -inf],\n",
       "        [0.1667, 0.2602, 0.2577,   -inf,   -inf,   -inf],\n",
       "        [0.0510, 0.1080, 0.1064, 0.0643,   -inf,   -inf],\n",
       "        [0.1415, 0.1875, 0.1863, 0.0987, 0.1121,   -inf],\n",
       "        [0.0476, 0.1192, 0.1171, 0.0731, 0.0477, 0.0966]],\n",
       "       grad_fn=<MaskedFillBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked_attention_scores = attention_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "masked_attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ce6f12d9-f07b-43ef-a7c3-ce312ac238b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3190, 0.3408, 0.3402, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2445, 0.2545, 0.2542, 0.2468, 0.0000, 0.0000],\n",
       "        [0.1994, 0.2060, 0.2058, 0.1935, 0.1953, 0.0000],\n",
       "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_attention_weights = torch.softmax(masked_attention_scores /  keys.shape[1]**0.5, dim=-1)\n",
    "masked_attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1917d28-d886-4582-9f81-ce5e401318d4",
   "metadata": {},
   "source": [
    "# Masking additional attentional weights with dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "187d520b-7ec7-46ca-8803-ef34db6d55e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout_layer = torch.nn.Dropout(0.5)\n",
    "example = torch.ones(6, 6)\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "625080db-abb5-4970-9ab1-aa3ee97cad5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2., 0., 2., 2., 0.],\n",
       "        [0., 0., 0., 2., 0., 2.],\n",
       "        [2., 2., 2., 2., 0., 2.],\n",
       "        [0., 2., 2., 0., 0., 2.],\n",
       "        [0., 2., 0., 2., 0., 2.],\n",
       "        [0., 2., 2., 2., 2., 0.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout_layer(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fd901da8-444f-43ed-9460-550a750f3607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.6816, 0.6804, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.5085, 0.4936, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.3906, 0.0000],\n",
       "        [0.3249, 0.3418, 0.0000, 0.3308, 0.3249, 0.3363]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout_layer(masked_attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bb14af-9c7b-47bb-9816-568afe0c5745",
   "metadata": {},
   "source": [
    "# Compact Causal self attention class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ae33d423-3072-4dbe-a129-7e93275aec20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 3])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e1570108-ff32-4b16-b020-6795259dc40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CausalAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.W_query = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout_layer = torch.nn.Dropout(dropout)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_tokens, d_in = x.shape\n",
    "        querys = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attention_scores = querys @ keys.transpose(1,2)\n",
    "        attention_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        attention_weights = torch.softmax(attention_scores /  keys.shape[-1]**0.5, dim=-1)\n",
    "        attention_weights = self.dropout_layer(attention_weights)\n",
    "\n",
    "        context_vectors = attention_weights @ values\n",
    "\n",
    "        return context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ac5a9c7a-daf3-4654-873c-16f5a708e4b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0872,  0.0286],\n",
       "         [-0.0991,  0.0501],\n",
       "         [-0.0999,  0.0633],\n",
       "         [-0.0983,  0.0489],\n",
       "         [-0.0514,  0.1098],\n",
       "         [-0.0754,  0.0693]],\n",
       "\n",
       "        [[-0.0872,  0.0286],\n",
       "         [-0.0991,  0.0501],\n",
       "         [-0.0999,  0.0633],\n",
       "         [-0.0983,  0.0489],\n",
       "         [-0.0514,  0.1098],\n",
       "         [-0.0754,  0.0693]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "context_length = batch.shape[1]\n",
    "dropout = 0.0\n",
    "causal_attention = CausalAttention(d_in, d_out,context_length, dropout)\n",
    "causal_attention(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da551174-46ff-4991-8a26-1150a472e7bb",
   "metadata": {},
   "source": [
    "# Extending single head attention to multi head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "11537d74-172a-4eaa-b95d-747810890fab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
       "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
       "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
       "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
       "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
       "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
       "\n",
       "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
       "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
       "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
       "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
       "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
       "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out,context_length, dropout, num_heads=2, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([CausalAttention(d_in, d_out,context_length, dropout, qkv_bias) for _ in range(num_heads)])\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "      return torch.cat([head(x)  for head in self.heads], dim=-1)\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "d_in = batch.shape[2]\n",
    "d_out = 2\n",
    "dropout = 0.0\n",
    "\n",
    "multi_head_attention_stacked = MultiHeadAttentionWrapper(d_in, d_out,context_length, dropout, num_heads=2)\n",
    "multi_head_attention_stacked(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8f9c1b-5d31-4140-b754-34ac0dc208af",
   "metadata": {},
   "source": [
    "# Multihead Attention with weight splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "76fb0846-4166-44a8-803f-69e3a4c535f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 6, 2])\n",
      "torch.Size([2, 2, 6, 6])\n",
      "torch.Size([2, 2, 6, 6])\n",
      "torch.Size([2, 6, 2, 2])\n",
      "torch.Size([2, 6, 4])\n",
      "torch.Size([2, 6, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3132, -0.2272,  0.4772,  0.1063],\n",
       "         [-0.2308,  0.0329,  0.5764,  0.3007],\n",
       "         [-0.2059,  0.1190,  0.6097,  0.3654],\n",
       "         [-0.1642,  0.1340,  0.5431,  0.3503],\n",
       "         [-0.1689,  0.1794,  0.5296,  0.3389],\n",
       "         [-0.1407,  0.1699,  0.5040,  0.3403]],\n",
       "\n",
       "        [[-0.3132, -0.2272,  0.4772,  0.1063],\n",
       "         [-0.2308,  0.0329,  0.5764,  0.3007],\n",
       "         [-0.2059,  0.1190,  0.6097,  0.3654],\n",
       "         [-0.1642,  0.1340,  0.5431,  0.3503],\n",
       "         [-0.1689,  0.1794,  0.5296,  0.3389],\n",
       "         [-0.1407,  0.1699,  0.5040,  0.3403]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out,context_length, dropout, num_heads=2, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert(d_out % num_heads == 0), \\\n",
    "        \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.W_query = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = torch.nn.Linear(d_out, d_out)\n",
    "        self.dropout_layer = torch.nn.Dropout(dropout)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_tokens, d_in = x.shape\n",
    "\n",
    "        queries = self.W_query(x)  #(batch_size, num_tokens, d_out)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        queries = queries.view(batch_size, num_tokens, self.num_heads, self.head_dim)   #(batch_size, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(batch_size, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(batch_size, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        queries = queries.transpose(1, 2)   #(batch_size, num_heads, num_tokens, head_dim)\n",
    "        print(queries.shape)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        attention_scores = queries @ keys.transpose(2, 3) #dot product for each head\n",
    "        print(attention_scores.shape)\n",
    "\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        attention_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attention_weights = torch.softmax(attention_scores /  keys.shape[-1]**0.5, dim=-1)\n",
    "        attention_weights = self.dropout_layer(attention_weights)\n",
    "        print(attention_weights.shape)\n",
    "\n",
    "        context_vectors = (attention_weights @ values).transpose(1,2) #(batch_size, num_tokens, num_heads, head_dim)\n",
    "        print(context_vectors.shape)\n",
    "\n",
    "        \n",
    "        # self.d_out = self.num_heads * self.head_dim\n",
    "        context_vectors = context_vectors.contiguous().view(batch_size, num_tokens, self.d_out)\n",
    "        print(context_vectors.shape)\n",
    "        context_vectors = self.out_proj(context_vectors) #optional projection\n",
    "        print(context_vectors.shape)\n",
    "\n",
    "        return context_vectors\n",
    "\n",
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "d_in = batch.shape[2]\n",
    "d_out = 4\n",
    "dropout = 0.0\n",
    "\n",
    "multi_head_attention_stacked = MultiHeadAttention(d_in, d_out,context_length, dropout, num_heads=2)\n",
    "multi_head_attention_stacked(batch)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15643805-e0f9-4ada-b8c0-3bf476b81506",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
